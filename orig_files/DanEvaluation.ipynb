{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IxXB4ZoRqDYC",
    "outputId": "8ab5fb83-adad-46d2-a5f4-168038169fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') \n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import pickle as pk\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def updateTargetGraph(tfVars,tau=0.9):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+12].assign((var.value()*tau) + ((1-tau)*tfVars[idx+12].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTargetMGraph(tfVars, tau=0.9):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+14].assign((var.value()*tau)) + (1-tau)*tfVars[idx+14].value())\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "    #total_vars = len(tf.trainable_variables())\n",
    "    #a = tf.trainable_variables()[0].eval(session=sess)\n",
    "    #b = tf.trainable_variables()[12].eval(session=sess)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzoBqpgaqHyK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "class SSenvReal():\n",
    "    \n",
    "    def __init__(self,xory):\n",
    "      \n",
    "        self.nStates = 21 #(these are the number of cells possible in the x space)\n",
    "        self.nActions = 10 # (0 to 9)\n",
    "        self.testOrTrain = 'train'\n",
    "        self.k = 0\n",
    "        self.index = 0\n",
    "        self.xory = xory\n",
    "        self.max_epLength = 16\n",
    "        self.track = self.getNewTrack()\n",
    "        self.state = self.get_next_state(self.track, self.k)\n",
    "        self.cc_by_sensors = self.get_covered_cells_by_sensors()\n",
    "        self.obsmat = [0.96,0.93,0.91,0.88,0.85,0.81,0.78,0.75,0.72,0.7]\n",
    "        \n",
    "    def discretize_cell(self,contstate): \n",
    "        \n",
    "        xcell = int((contstate[0])*2/15)\n",
    "        ycell = int((contstate[1])*2/15)\n",
    "        discstate = [xcell, ycell]\n",
    "        return discstate\n",
    "\n",
    "\n",
    "    def get_next_state(self,track,index):\n",
    "        \n",
    "        index = index + 1\n",
    "        if index < len(track[0]):\n",
    "            return [track[0][index], track[1][index]]\n",
    "        else:\n",
    "            return [21,21]\n",
    "       \n",
    "    def getNewTrack(self):\n",
    "\n",
    "        return self.load_track()\n",
    "        \n",
    "    def load_track(self):\n",
    "        \n",
    "        if self.xory == 'x':\n",
    "          # sx = pk.load(open('/content/drive/My Drive/Dancode/sampled_tracksX','rb'))\n",
    "          sx = pk.load(open('../data/sampled_tracksX','rb'))\n",
    "        elif self.xory == 'y':\n",
    "          # sx = pk.load(open('/content/drive/My Drive/Dancode/sampled_tracksX','rb'))\n",
    "          sx = pk.load(open('../data/sampled_tracksX','rb'))\n",
    "        tracknu = np.random.randint(0,len(sx))\n",
    "        trck = sx[tracknu]\n",
    "        \n",
    "        return trck\n",
    "        \n",
    "        \n",
    "    def get_cell(self,xypoint): \n",
    "        \n",
    "        xp = xypoint[0] + 50\n",
    "        yp = xypoint[1] + 50\n",
    "        xcell = xp//1\n",
    "        ycell = yp//1\n",
    "        return [xcell,ycell]\n",
    "        \n",
    "        \n",
    "    def get_covered_cells_by_sensors(self):\n",
    "        \n",
    "        mat_content = sio.loadmat('../data/dandc.mat')\n",
    "        cell_info = mat_content['c2']\n",
    "        covered_cells_by_sensors = {}\n",
    "        for i in range(10):\n",
    "            covered_cells_by_sensors[i] = cell_info[0][i][0].tolist()\n",
    "                \n",
    "        for i in range(10):\n",
    "            covered_cells_by_sensors[i].extend(cell_info[0][i+10][0].tolist())\n",
    "   \n",
    "        return covered_cells_by_sensors\n",
    "\n",
    "\n",
    "    def get_obs(self, cellstate, action):\n",
    "#         print('cellstate', cellstate) # this is already discretized\n",
    "#         print('action', action)\n",
    "#         print(\"self.cc_by_sensors\", self.cc_by_sensors)\n",
    "#         print(\"######\")\n",
    "#         print(self.cc_by_sensors[action])\n",
    "#         input()\n",
    "        cc_by_s = self.cc_by_sensors[action]\n",
    "        \n",
    "        if cellstate in cc_by_s:\n",
    "            temp =  self.discretize_cell(cellstate)\n",
    "            return temp\n",
    "        else:\n",
    "            return [21,21] \n",
    "    \n",
    "    def get_obsX(self, cellstate, action):\n",
    "        \n",
    "        temp = self.get_obs(cellstate, action)\n",
    "        if np.random.rand(1) < self.obsmat[action]:\n",
    "            return temp[0]\n",
    "        else:\n",
    "            return 21\n",
    "    \n",
    "    def get_obsY(self, cellstate, action):\n",
    "        temp = self.get_obs(cellstate, action)\n",
    "        if np.random.rand(1) < self.obsmat[action]:\n",
    "            return temp[1]\n",
    "        else:\n",
    "            return 21\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MT8NvYe13G7x"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Qnetwork():\n",
    "    \n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    \n",
    "    def __init__(self,h_size,nStates,nActions,rnn_cell,myScope):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.inpSize = nStates + nActions\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,self.inpSize],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,1,self.inpSize])\n",
    "        \n",
    "        with tf.variable_scope(myScope+\"_fclayers\"):\n",
    "            self.weight1 = self.weight_variable([self.inpSize,40])\n",
    "            self.bias1 = self.bias_variable([40])\n",
    "            \n",
    "            self.weight11 = self.weight_variable([40,20])\n",
    "            self.bias11 = self.bias_variable([20])\n",
    "\n",
    "            self.weight2 = self.weight_variable([20,h_size])\n",
    "            self.bias2 = self.bias_variable([h_size])\n",
    "\n",
    "            self.wfin = self.weight_variable([h_size,nActions])\n",
    "            self.bfin = self.bias_variable([nActions])\n",
    "        \n",
    "        self.hstate1 = (tf.matmul(self.scalarInput, self.weight1) + self.bias1)\n",
    "        self.hstate11 = tf.matmul(self.hstate1, self.weight11) + self.bias11\n",
    "        self.hstate2 = tf.matmul(self.hstate11, self.weight2) + self.bias2\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)        \n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.hstate2),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "        self.streamA,self.streamV = tf.split(self.rnn,2,1)\n",
    "        with tf.variable_scope(myScope+\"_fclayers\"):\n",
    "            self.AW = tf.Variable(tf.random_normal([h_size//2,nActions]))\n",
    "            self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keepdims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,nActions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "        #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "        self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vgpTq6l3J36"
   },
   "outputs": [],
   "source": [
    "class Mnetwork():\n",
    "    \n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    \n",
    "    def __init__(self,h_size,nStates,nActions,rnn_cell,myScope):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.inpSize = nStates + nActions\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,inpSize],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,1,inpSize])\n",
    "        #self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        #self.actions_onehot = tf.one_hot(self.actions,nStates,dtype=tf.float32)\n",
    "        #self.newScalarInput = tf.concat([self.scalarInput, self.actions_onehot],1)\n",
    "        with tf.variable_scope(myScope+\"_fclayers\"):\n",
    "            self.weight1 = self.weight_variable([inpSize,40])\n",
    "            self.bias1 = self.bias_variable([40])\n",
    "\n",
    "            self.weight11 = self.weight_variable([40,20])\n",
    "            self.bias11 = self.bias_variable([20])\n",
    "            \n",
    "            self.weight2 = self.weight_variable([20,10])\n",
    "            self.bias2 = self.bias_variable([10])\n",
    "            \n",
    "            self.weight3 = self.weight_variable([10,h_size])\n",
    "            self.bias3 = self.bias_variable([h_size])\n",
    "            \n",
    "            self.weight4 = self.weight_variable([h_size,40])\n",
    "            self.bias4 = self.bias_variable([40])\n",
    "            \n",
    "            self.wfin = self.weight_variable([40,nStates])\n",
    "            self.bfin = self.bias_variable([nStates])\n",
    "        \n",
    "        self.hstate1 = (tf.matmul(self.scalarInput, self.weight1) + self.bias1)\n",
    "        self.hstate11 = tf.matmul(self.hstate1, self.weight11) + self.bias11\n",
    "        self.hstate2 = tf.matmul(self.hstate11, self.weight2) + self.bias2\n",
    "        self.hstate3 = tf.matmul(self.hstate2, self.weight3) + self.bias3\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)        \n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.hstate3),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        \n",
    "        self.prepred = tf.matmul(self.rnn,self.weight4) + self.bias4 \n",
    "        self.prediction = tf.matmul(self.prepred, self.wfin) + self.bfin\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetP = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.targetP_onehot = tf.one_hot(self.targetP, nStates, dtype=tf.float32)\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.targetP_onehot,logits=self.prediction))\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdpXrZVS3LZF"
   },
   "outputs": [],
   "source": [
    "class experience_buffer:\n",
    "    def __init__(self, seed, buffer_size = 20000):\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        # sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        idx = self.rng.choice(len(self.buffer),batch_size)\n",
    "        sampled_episodes = [self.buffer[i] for i in idx]\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVSa3qTBeIYF"
   },
   "outputs": [],
   "source": [
    "def get_pred_reward(prediction, ns_cell):\n",
    "  \n",
    "  if np.argmax(prediction)==ns_cell:\n",
    "      r = 1\n",
    "  else:\n",
    "      r = 0\n",
    "  return r\n",
    "\n",
    "def get_cover_reward(obs):\n",
    "  if obs < 21:\n",
    "    r = 1\n",
    "  else:\n",
    "    r = 0\n",
    "  return r\n",
    "  \n",
    "\n",
    "def format_obs(obs,nStates,a_onehot):\n",
    "  \n",
    "  obs_onehot = np.reshape(np.array([int(i == obs) for i in range(nStates)]),[1,nStates])\n",
    "  fin_obs = np.reshape(np.append(obs_onehot, a_onehot), [1,inpSize])\n",
    "  return fin_obs\n",
    "\n",
    "def get_Qaction(mainQN,state,sess,prev_obs):\n",
    "\n",
    "  if len(prev_obs)==1:\n",
    "#     print(\"agent_start\")\n",
    "#     print(\"prev_obs: {}\".format(prev_obs))\n",
    "#     input()\n",
    "    stateQx = state\n",
    "    a = np.random.randint(0,sse.nActions)\n",
    "    return a,stateQx\n",
    "  else:\n",
    "#     print(\"agent step\")\n",
    "#     print(\"prev_obs: {}\".format(prev_obs))\n",
    "#     input()\n",
    "    Q, stateQ = sess.run([mainQN.Qout, mainQN.rnn_state],\\\n",
    "          feed_dict={mainQN.scalarInput:prev_obs,mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "    a = np.argmax(Q,1)\n",
    "    a = a[0]\n",
    "    return a,stateQ\n",
    "  \n",
    "def run_episode(sse,mainQN,mainMN,sess,xory):\n",
    "  \n",
    "  prev_obsx = np.reshape([0]*(sse.nStates+sse.nActions),[1,31])\n",
    "  state = (np.zeros([1,h_size]),np.zeros([1,h_size])) \n",
    "  stateQx = state\n",
    "  stateMx = state\n",
    "  episodeBuffer = []\n",
    "  d = False\n",
    "  rew = 0\n",
    "  for i in range(0,sse.max_epLength):\n",
    "\n",
    "    a,stateQx = get_Qaction(mainQN,stateQx,sess,prev_obsx)\n",
    "    a_onehot = np.reshape(np.array([int(i == a) for i in range(sse.nActions)]),[1,sse.nActions])\n",
    "    ns = (sse.get_cell(sse.get_next_state(sse.track, sse.index)))\n",
    "    print('sse.index', sse.index)\n",
    "    input()\n",
    "    ns_cell = sse.discretize_cell(ns)\n",
    "    x_cell = ns_cell[0]\n",
    "    y_cell = ns_cell[1]\n",
    "    if xory=='x':\n",
    "      obsx = np.reshape(sse.get_obsX(ns,a),[1,1])\n",
    "      curr_obsx = format_obs(obsx,sse.nStates,a_onehot)\n",
    "      prediction,stateMx = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsx,mainMN.trainLength:1,mainMN.state_in:stateMx,mainMN.batch_size:1})\n",
    "      r = get_pred_reward(prediction,x_cell)\n",
    "      episodeBuffer.append(np.reshape(np.array([prev_obsx,a,r,curr_obsx,d,x_cell]),[1,6]))\n",
    "    elif xory=='y':\n",
    "      obsx = np.reshape(sse.get_obsY(ns,a),[1,1])\n",
    "      curr_obsx = format_obs(obsx,sse.nStates,a_onehot)\n",
    "      prediction,stateMx = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsx,mainMN.trainLength:1,mainMN.state_in:stateMx,mainMN.batch_size:1})\n",
    "      r = get_pred_reward(prediction,y_cell)\n",
    "      episodeBuffer.append(np.reshape(np.array([prev_obsx,a,r,curr_obsx,d,y_cell]),[1,6]))      \n",
    "    rew = rew + r\n",
    "    prev_obsx = curr_obsx\n",
    "\n",
    "  return episodeBuffer,rew\n",
    "\n",
    "def updateQandM(myBuffer,mainQN,mainMN):\n",
    "  state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size]))  \n",
    "  trainBatch = myBuffer.sample(batch_size,trace_length) #Get a random batch of experiences.\n",
    "  #Below we perform the Double-DQN update to the target Q-values\n",
    "  Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "      mainQN.scalarInput:np.vstack(trainBatch[:,3]), \\\n",
    "      mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "  Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "      targetQN.scalarInput:np.vstack(trainBatch[:,3]), \\\n",
    "      targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n",
    "  end_multiplier = -(trainBatch[:,4] - 1)\n",
    "  doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "  targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "  sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]), mainQN.targetQ:targetQ,\\\n",
    "                        mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                        mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "   \n",
    "  sess.run(mainMN.updateModel, \\\n",
    "                        feed_dict={mainMN.scalarInput:np.vstack(trainBatch[:,3]),mainMN.targetP:trainBatch[:,5],\\\n",
    "                        mainMN.trainLength:trace_length,\\\n",
    "                        mainMN.state_in:state_train,mainMN.batch_size:batch_size})\n",
    "  return mainQN, mainMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKOcUho63hoE"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 #How many experience traces to use for each training step.\n",
    "trace_length = 4 #How long each experience trace will be when training\n",
    "update_freq = 2  #How often to perform a training step.\n",
    "update_target = 20\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 50000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 1000000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 5000  #How many steps of random actions before training begins.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./drqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final recurrent layer before splitting it into Advantage and Value streams.\n",
    "max_epLength = 16 #The max allowed length of our episode.\n",
    "time_per_step = 1 #Length of each step used in gif creation\n",
    "summaryLength = 25 #Number of epidoes to periodically save for analysis\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "ZEd-bjui324o",
    "outputId": "1cc31b03-9cf6-4616-ac78-f7dbbc0be141"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sse = SSenvReal('x')\n",
    "\n",
    "nActions = sse.nActions\n",
    "nStates = sse.nStates\n",
    "inpSize = nStates + nActions\n",
    "# file = open('/content/drive/My Drive/Dancode/testfileX.txt','w') \n",
    " \n",
    "#We define the cells for the primary and target q-networks for x coordinates\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,sse.nStates,sse.nActions,cell,'mainqx')\n",
    "targetQN = Qnetwork(h_size,sse.nStates,sse.nActions,cellT,'targetqx')\n",
    "\n",
    "mcell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "mcellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainMN = Mnetwork(h_size,sse.nStates,sse.nActions, mcell,'mmainx')\n",
    "targetMN = Mnetwork(h_size,sse.nStates,sse.nActions, mcellT,'mtargetx')\n",
    "\n",
    "#We define the same thing for Y\n",
    "cellY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellTY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQNY = Qnetwork(h_size,sse.nStates,sse.nActions,cellY,'mainqy')\n",
    "targetQNY = Qnetwork(h_size,sse.nStates,sse.nActions,cellTY,'targetqy')\n",
    "\n",
    "mcellY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "mcellTY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainMNY = Mnetwork(h_size,sse.nStates,sse.nActions, mcellY,'mmainy')\n",
    "targetMNY = Mnetwork(h_size,sse.nStates,sse.nActions, mcellTY,'mtargety')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITv5KvMlKyqz"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "trainables = tf.trainable_variables()\n",
    "variables_names = [v.name for v in tf.trainable_variables()]\n",
    "\n",
    "targetOps = updateTargetGraph(trainables[0:24],tau)\n",
    "targetOpsM = updateTargetMGraph(trainables[24:52], tau)\n",
    "targetOpsY = updateTargetGraph(trainables[52:76],tau)\n",
    "targetOpsMY = updateTargetMGraph(trainables[76:104],tau)\n",
    "\n",
    "\n",
    "myBuffer = experience_buffer(seed=0)\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2959
    },
    "colab_type": "code",
    "id": "trttsrde7ZAm",
    "outputId": "ad327a2e-5fab-42fc-e686-107483c32806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n",
      "\n",
      "sse.index 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-aab9a7fcafd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0msse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSSenvReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0msse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_epLength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mepbf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainMN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mbufferArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepbf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mepisodeBuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufferArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-f5cfbd073f42>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(sse, mainQN, mainMN, sess, xory)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sse.index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mns_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mx_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mns_cell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Here we are learning mainQ and mainM over the x-coordinates.\n",
    "seed = 0\n",
    "\n",
    "myBuffer = experience_buffer(seed)\n",
    "myBufferY = experience_buffer(seed)\n",
    "total_steps = 0\n",
    "total_eps = 0\n",
    "rList = []\n",
    "rListY = []\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  for ieps in range(1600):\n",
    "    \n",
    "    ### run an episode for x coord\n",
    "    if ieps%2 == 0:\n",
    "      sse = SSenvReal('x')\n",
    "      sse.max_epLength = max_epLength\n",
    "      epbf,rew = run_episode(sse,mainQN,mainMN,sess,'x')\n",
    "      bufferArray = np.array(epbf)\n",
    "      episodeBuffer = list(zip(bufferArray))\n",
    "      myBuffer.add(episodeBuffer)\n",
    "      rList.append(rew)\n",
    "\n",
    "    else:    \n",
    "    ### run an episode for Y coord\n",
    "      sse = SSenvReal('y')\n",
    "      sse.max_epLength = max_epLength\n",
    "      epbf,rew = run_episode(sse,mainQNY,mainMNY,sess,'y')\n",
    "      bufferArray = np.array(epbf)\n",
    "      episodeBuffer = list(zip(bufferArray))\n",
    "      myBufferY.add(episodeBuffer)\n",
    "      rListY.append(rew)\n",
    "\n",
    "    sse.index = sse.index + 1\n",
    "    total_steps += 16\n",
    "    total_eps += 1\n",
    "    \n",
    "    if total_eps % 5 == 0:\n",
    "      updateTarget(targetOps,sess)\n",
    "      updateTarget(targetOpsM,sess)\n",
    "      updateTarget(targetOpsY,sess)\n",
    "      updateTarget(targetOpsMY,sess)\n",
    "      #print('target updated')\n",
    "    \n",
    "    if total_eps > 20:\n",
    "      if total_eps % 2 == 0:\n",
    "        mainQN, mainMN = updateQandM(myBuffer,mainQN,mainMN)\n",
    "        mainQNY, mainMNY = updateQandM(myBufferY,mainQNY,mainMNY)\n",
    "        #print('model updated')\n",
    "    \n",
    "    if (len(rList) % summaryLength) == 0 and len(rList) != 0:\n",
    "      print (total_steps,np.mean(rList[-summaryLength:]))\n",
    "      print (total_steps,np.mean(rListY[-summaryLength:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uX-GPSyIv_Tp"
   },
   "outputs": [],
   "source": [
    "### What needs to be done ### \n",
    "\n",
    "# Use the function run_episodes to evaluate the policy learned on multi-person tracking (maybe also separate a small test data beforehand)\n",
    "# Use the function get_cover_reward to get a baseline and get learning curve and performance on multi person tracking\n",
    "# Get same performance for random policy (that is Q networks are never updated)\n",
    "# Finally, multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFi3o8YoLwCM"
   },
   "outputs": [],
   "source": [
    "def get_eval_action(mainQNX,mainQNY,state,sess,prev_obs):\n",
    "\n",
    "  if len(prev_obs)==1:\n",
    "    Qx,stateQx = sess.run([mainQNX.Qout, mainQNX.rnn_state],\\\n",
    "          feed_dict={mainQNX.scalarInput:prev_obs,mainQNX.trainLength:1,mainQNX.state_in:state,mainQNX.batch_size:1})\n",
    "    Qy,stateQy = sess.run([mainQNY.Qout, mainQNY.rnn_state],\\\n",
    "          feed_dict={mainQNY.scalarInput:prev_obs,mainQNY.trainLength:1,mainQNY.state_in:state,mainQNY.batch_size:1})\n",
    "    Q = Qx + Qy\n",
    "    a = np.argmax(Q,1)\n",
    "    a = a[0]\n",
    "    return a,stateQx,stateQy\n",
    "  \n",
    "  \n",
    "def run_evaluation_episode(mainQN,mainMN,mainQNY,mainMNY,sess):\n",
    "  \n",
    "  sse1 = SSenvReal()\n",
    "  sse2 = SSenvReal()\n",
    "  \n",
    "  prev_obsx = np.reshape([0]*(sse.nStates+sse.nActions),[1,31])\n",
    "  prev_obsy = np.reshape([0]*(sse.nStates+sse.nActions),[1,31])\n",
    "  state = (np.zeros([1,h_size]),np.zeros([1,h_size])) \n",
    "  stateQx = state\n",
    "  stateQy = state\n",
    "  stateMx = state\n",
    "  stateMy = state\n",
    "  episodeBuffer = []\n",
    "  d = False\n",
    "  rew = 0\n",
    "  for i in range(0,sse.max_epLength):\n",
    "\n",
    "    a,stateQx,stateQy = get_Qaction(mainQN,stateQx,sess,prev_obsx)\n",
    "    a_onehot = np.reshape(np.array([int(i == a) for i in range(sse.nActions)]),[1,sse.nActions])\n",
    "  \n",
    "    ns = (sse.get_cell(sse.get_next_state(sse.track, sse.index)))\n",
    "    ns_cell = sse.discretize_cell(ns)\n",
    "    x_cell = ns_cell[0]\n",
    "    y_cell = ns_cell[1]\n",
    "    \n",
    "    obsx = np.reshape(sse.get_obsX(ns,a),[1,1])\n",
    "    curr_obsx = format_obs(obsx,sse.nStates,a_onehot)\n",
    "    prediction,stateMx = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsx,mainMN.trainLength:1,mainMN.state_in:stateMx,mainMN.batch_size:1})\n",
    "    rx = get_pred_reward(prediction,x_cell)\n",
    "\n",
    "    obsy = np.reshape(sse.get_obsY(ns,a),[1,1])\n",
    "    curr_obsy = format_obs(obsy,sse.nStates,a_onehot)\n",
    "    prediction,stateMy = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsy,mainMN.trainLength:1,mainMN.state_in:stateMy,mainMN.batch_size:1})\n",
    "    ry = get_pred_reward(prediction,x_cell)\n",
    "  \n",
    "    rew = rew + rx + ry\n",
    "    sse1.index = sse1.index + 1\n",
    "    prev_obsx = curr_obsx\n",
    "    prev_obsy = curr_obsy\n",
    "  return rew\n",
    "\n",
    "\n",
    "# Here the idea is to use the trained QX, MX, QY and MY nets to run an evaluation on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzIq5T4SZ14_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DanEvaluation",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
