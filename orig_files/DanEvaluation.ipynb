{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IxXB4ZoRqDYC",
    "outputId": "8ab5fb83-adad-46d2-a5f4-168038169fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f952b4f214d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') \n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import pickle as pk\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def updateTargetGraph(tfVars,tau=0.9):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+12].assign((var.value()*tau) + ((1-tau)*tfVars[idx+12].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTargetMGraph(tfVars, tau=0.9):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+14].assign((var.value()*tau)) + (1-tau)*tfVars[idx+14].value())\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "    #total_vars = len(tf.trainable_variables())\n",
    "    #a = tf.trainable_variables()[0].eval(session=sess)\n",
    "    #b = tf.trainable_variables()[12].eval(session=sess)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzoBqpgaqHyK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "class SSenvReal():\n",
    "    \n",
    "    def __init__(self,xory):\n",
    "      \n",
    "        self.nStates = 21 #(these are the number of cells possible in the x space)\n",
    "        self.nActions = 10 # (0 to 9)\n",
    "        self.testOrTrain = 'train'\n",
    "        self.k = 0\n",
    "        self.index = 0\n",
    "        self.xory = xory\n",
    "        self.max_epLength = 16\n",
    "        self.track = self.getNewTrack()\n",
    "        self.state = self.get_next_state(self.track, self.k)\n",
    "        self.cc_by_sensors = self.get_covered_cells_by_sensors()\n",
    "        self.obsmat = [0.96,0.93,0.91,0.88,0.85,0.81,0.78,0.75,0.72,0.7]\n",
    "        \n",
    "    def discretize_cell(self,contstate): \n",
    "        \n",
    "        xcell = int((contstate[0])*2/15)\n",
    "        ycell = int((contstate[1])*2/15)\n",
    "        discstate = [xcell, ycell]\n",
    "        return discstate\n",
    "\n",
    "\n",
    "    def get_next_state(self,track,index):\n",
    "        \n",
    "        index = index + 1\n",
    "        if index < len(track[0]):\n",
    "            return [track[0][index], track[1][index]]\n",
    "        else:\n",
    "            return [21,21]\n",
    "       \n",
    "    def getNewTrack(self):\n",
    "\n",
    "        return self.load_track()\n",
    "        \n",
    "    def load_track(self):\n",
    "        \n",
    "        if self.xory == 'x':\n",
    "          sx = pk.load(open('/content/drive/My Drive/Dancode/sampled_tracksX','rb'))\n",
    "        elif self.xory == 'y':\n",
    "          sx = pk.load(open('/content/drive/My Drive/Dancode/sampled_tracksX','rb'))\n",
    "        tracknu = np.random.randint(0,len(sx))\n",
    "        trck = sx[tracknu]\n",
    "        \n",
    "        return trck\n",
    "        \n",
    "        \n",
    "    def get_cell(self,xypoint): \n",
    "        \n",
    "        xp = xypoint[0] + 50\n",
    "        yp = xypoint[1] + 50\n",
    "        xcell = xp//1\n",
    "        ycell = yp//1\n",
    "        return [xcell,ycell]\n",
    "        \n",
    "        \n",
    "    def get_covered_cells_by_sensors(self):\n",
    "        \n",
    "        mat_content = sio.loadmat('/content/drive/My Drive/Dancode/dandc.mat')\n",
    "        cell_info = mat_content['c2']\n",
    "        covered_cells_by_sensors = {}\n",
    "        for i in range(10):\n",
    "            covered_cells_by_sensors[i] = cell_info[0][i][0].tolist()\n",
    "                \n",
    "        for i in range(10):\n",
    "            covered_cells_by_sensors[i].extend(cell_info[0][i+10][0].tolist())\n",
    "   \n",
    "        return covered_cells_by_sensors\n",
    "\n",
    "\n",
    "    def get_obs(self, cellstate, action):\n",
    "        \n",
    "        cc_by_s = self.cc_by_sensors[action]\n",
    "        \n",
    "        if cellstate in cc_by_s:\n",
    "            temp =  self.discretize_cell(cellstate)\n",
    "            return temp\n",
    "        else:\n",
    "            return [21,21] \n",
    "    \n",
    "    def get_obsX(self, cellstate, action):\n",
    "        \n",
    "        temp = self.get_obs(cellstate, action)\n",
    "        if np.random.rand(1) < self.obsmat[action]:\n",
    "            return temp[0]\n",
    "        else:\n",
    "            return 21\n",
    "    \n",
    "    def get_obsY(self, cellstate, action):\n",
    "        temp = self.get_obs(cellstate, action)\n",
    "        if np.random.rand(1) < self.obsmat[action]:\n",
    "            return temp[1]\n",
    "        else:\n",
    "            return 21\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MT8NvYe13G7x"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Qnetwork():\n",
    "    \n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    \n",
    "    def __init__(self,h_size,nStates,nActions,rnn_cell,myScope):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.inpSize = nStates + nActions\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,self.inpSize],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,1,self.inpSize])\n",
    "        \n",
    "        with tf.variable_scope(myScope+\"_fclayers\"):\n",
    "            self.weight1 = self.weight_variable([self.inpSize,40])\n",
    "            self.bias1 = self.bias_variable([40])\n",
    "            \n",
    "            self.weight11 = self.weight_variable([40,20])\n",
    "            self.bias11 = self.bias_variable([20])\n",
    "\n",
    "            self.weight2 = self.weight_variable([20,h_size])\n",
    "            self.bias2 = self.bias_variable([h_size])\n",
    "\n",
    "            self.wfin = self.weight_variable([h_size,nActions])\n",
    "            self.bfin = self.bias_variable([nActions])\n",
    "        \n",
    "        self.hstate1 = (tf.matmul(self.scalarInput, self.weight1) + self.bias1)\n",
    "        self.hstate11 = tf.matmul(self.hstate1, self.weight11) + self.bias11\n",
    "        self.hstate2 = tf.matmul(self.hstate11, self.weight2) + self.bias2\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)        \n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.hstate2),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "        self.streamA,self.streamV = tf.split(self.rnn,2,1)\n",
    "        with tf.variable_scope(myScope+\"_fclayers\"):\n",
    "            self.AW = tf.Variable(tf.random_normal([h_size//2,nActions]))\n",
    "            self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keepdims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,nActions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "        #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "        self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vgpTq6l3J36"
   },
   "outputs": [],
   "source": [
    "class Mnetwork():\n",
    "    \n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    \n",
    "    def __init__(self,h_size,nStates,nActions,rnn_cell,myScope):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.inpSize = nStates + nActions\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,inpSize],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,1,inpSize])\n",
    "        #self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        #self.actions_onehot = tf.one_hot(self.actions,nStates,dtype=tf.float32)\n",
    "        #self.newScalarInput = tf.concat([self.scalarInput, self.actions_onehot],1)\n",
    "        with tf.variable_scope(myScope+\"_fclayers\"):\n",
    "            self.weight1 = self.weight_variable([inpSize,40])\n",
    "            self.bias1 = self.bias_variable([40])\n",
    "\n",
    "            self.weight11 = self.weight_variable([40,20])\n",
    "            self.bias11 = self.bias_variable([20])\n",
    "            \n",
    "            self.weight2 = self.weight_variable([20,10])\n",
    "            self.bias2 = self.bias_variable([10])\n",
    "            \n",
    "            self.weight3 = self.weight_variable([10,h_size])\n",
    "            self.bias3 = self.bias_variable([h_size])\n",
    "            \n",
    "            self.weight4 = self.weight_variable([h_size,40])\n",
    "            self.bias4 = self.bias_variable([40])\n",
    "            \n",
    "            self.wfin = self.weight_variable([40,nStates])\n",
    "            self.bfin = self.bias_variable([nStates])\n",
    "        \n",
    "        self.hstate1 = (tf.matmul(self.scalarInput, self.weight1) + self.bias1)\n",
    "        self.hstate11 = tf.matmul(self.hstate1, self.weight11) + self.bias11\n",
    "        self.hstate2 = tf.matmul(self.hstate11, self.weight2) + self.bias2\n",
    "        self.hstate3 = tf.matmul(self.hstate2, self.weight3) + self.bias3\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)        \n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.hstate3),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        \n",
    "        self.prepred = tf.matmul(self.rnn,self.weight4) + self.bias4 \n",
    "        self.prediction = tf.matmul(self.prepred, self.wfin) + self.bfin\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetP = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.targetP_onehot = tf.one_hot(self.targetP, nStates, dtype=tf.float32)\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.targetP_onehot,logits=self.prediction))\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdpXrZVS3LZF"
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 20000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVSa3qTBeIYF"
   },
   "outputs": [],
   "source": [
    "def get_pred_reward(prediction, ns_cell):\n",
    "  \n",
    "  if np.argmax(prediction)==ns_cell:\n",
    "      r = 1\n",
    "  else:\n",
    "      r = 0\n",
    "  return r\n",
    "\n",
    "def get_cover_reward(obs):\n",
    "  if obs < 21:\n",
    "    r = 1\n",
    "  else:\n",
    "    r = 0\n",
    "  return r\n",
    "  \n",
    "\n",
    "def format_obs(obs,nStates,a_onehot):\n",
    "  \n",
    "  obs_onehot = np.reshape(np.array([int(i == obs) for i in range(nStates)]),[1,nStates])\n",
    "  fin_obs = np.reshape(np.append(obs_onehot, a_onehot), [1,inpSize])\n",
    "  return fin_obs\n",
    "\n",
    "def get_Qaction(mainQN,state,sess,prev_obs):\n",
    "\n",
    "  if len(prev_obs)==1:\n",
    "    stateQx = state\n",
    "    a = np.random.randint(0,sse.nActions)\n",
    "    return a,stateQx\n",
    "  else:\n",
    "    Q, stateQ = sess.run([mainQN.Qout, mainQN.rnn_state],\\\n",
    "          feed_dict={mainQN.scalarInput:prev_obs,mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "    a = np.argmax(Q,1)\n",
    "    a = a[0]\n",
    "    return a,stateQ\n",
    "  \n",
    "def run_episode(sse,mainQN,mainMN,sess,xory):\n",
    "  \n",
    "  prev_obsx = np.reshape([0]*(sse.nStates+sse.nActions),[1,31])\n",
    "  state = (np.zeros([1,h_size]),np.zeros([1,h_size])) \n",
    "  stateQx = state\n",
    "  stateMx = state\n",
    "  episodeBuffer = []\n",
    "  d = False\n",
    "  rew = 0\n",
    "  for i in range(0,sse.max_epLength):\n",
    "\n",
    "    a,stateQx = get_Qaction(mainQN,stateQx,sess,prev_obsx)\n",
    "    a_onehot = np.reshape(np.array([int(i == a) for i in range(sse.nActions)]),[1,sse.nActions])\n",
    "  \n",
    "    ns = (sse.get_cell(sse.get_next_state(sse.track, sse.index)))\n",
    "    ns_cell = sse.discretize_cell(ns)\n",
    "    x_cell = ns_cell[0]\n",
    "    y_cell = ns_cell[1]\n",
    "    if xory=='x':\n",
    "      obsx = np.reshape(sse.get_obsX(ns,a),[1,1])\n",
    "      curr_obsx = format_obs(obsx,sse.nStates,a_onehot)\n",
    "      prediction,stateMx = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsx,mainMN.trainLength:1,mainMN.state_in:stateMx,mainMN.batch_size:1})\n",
    "      r = get_pred_reward(prediction,x_cell)\n",
    "      episodeBuffer.append(np.reshape(np.array([prev_obsx,a,r,curr_obsx,d,x_cell]),[1,6]))\n",
    "    elif xory=='y':\n",
    "      obsx = np.reshape(sse.get_obsY(ns,a),[1,1])\n",
    "      curr_obsx = format_obs(obsx,sse.nStates,a_onehot)\n",
    "      prediction,stateMx = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsx,mainMN.trainLength:1,mainMN.state_in:stateMx,mainMN.batch_size:1})\n",
    "      r = get_pred_reward(prediction,y_cell)\n",
    "      episodeBuffer.append(np.reshape(np.array([prev_obsx,a,r,curr_obsx,d,y_cell]),[1,6]))      \n",
    "    rew = rew + r\n",
    "    prev_obsx = curr_obsx\n",
    "\n",
    "  return episodeBuffer,rew\n",
    "\n",
    "def updateQandM(myBuffer,mainQN,mainMN):\n",
    "  state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size]))  \n",
    "  trainBatch = myBuffer.sample(batch_size,trace_length) #Get a random batch of experiences.\n",
    "  #Below we perform the Double-DQN update to the target Q-values\n",
    "  Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "      mainQN.scalarInput:np.vstack(trainBatch[:,3]), \\\n",
    "      mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "  Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "      targetQN.scalarInput:np.vstack(trainBatch[:,3]), \\\n",
    "      targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n",
    "  end_multiplier = -(trainBatch[:,4] - 1)\n",
    "  doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "  targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "  sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]), mainQN.targetQ:targetQ,\\\n",
    "                        mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                        mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "   \n",
    "  sess.run(mainMN.updateModel, \\\n",
    "                        feed_dict={mainMN.scalarInput:np.vstack(trainBatch[:,3]),mainMN.targetP:trainBatch[:,5],\\\n",
    "                        mainMN.trainLength:trace_length,\\\n",
    "                        mainMN.state_in:state_train,mainMN.batch_size:batch_size})\n",
    "  return mainQN, mainMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKOcUho63hoE"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 #How many experience traces to use for each training step.\n",
    "trace_length = 4 #How long each experience trace will be when training\n",
    "update_freq = 2  #How often to perform a training step.\n",
    "update_target = 20\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 50000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 1000000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 5000  #How many steps of random actions before training begins.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./drqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final recurrent layer before splitting it into Advantage and Value streams.\n",
    "max_epLength = 16 #The max allowed length of our episode.\n",
    "time_per_step = 1 #Length of each step used in gif creation\n",
    "summaryLength = 25 #Number of epidoes to periodically save for analysis\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "ZEd-bjui324o",
    "outputId": "1cc31b03-9cf6-4616-ac78-f7dbbc0be141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-d10b5553394c>:10: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-668c4d20b970>:41: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sse = SSenvReal('x')\n",
    "\n",
    "nActions = sse.nActions\n",
    "nStates = sse.nStates\n",
    "inpSize = nStates + nActions\n",
    "file = open('/content/drive/My Drive/Dancode/testfileX.txt','w') \n",
    " \n",
    "#We define the cells for the primary and target q-networks for x coordinates\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,sse.nStates,sse.nActions,cell,'mainqx')\n",
    "targetQN = Qnetwork(h_size,sse.nStates,sse.nActions,cellT,'targetqx')\n",
    "\n",
    "mcell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "mcellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainMN = Mnetwork(h_size,sse.nStates,sse.nActions, mcell,'mmainx')\n",
    "targetMN = Mnetwork(h_size,sse.nStates,sse.nActions, mcellT,'mtargetx')\n",
    "\n",
    "#We define the same thing for Y\n",
    "cellY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellTY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQNY = Qnetwork(h_size,sse.nStates,sse.nActions,cellY,'mainqy')\n",
    "targetQNY = Qnetwork(h_size,sse.nStates,sse.nActions,cellTY,'targetqy')\n",
    "\n",
    "mcellY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "mcellTY = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainMNY = Mnetwork(h_size,sse.nStates,sse.nActions, mcellY,'mmainy')\n",
    "targetMNY = Mnetwork(h_size,sse.nStates,sse.nActions, mcellTY,'mtargety')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITv5KvMlKyqz"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "trainables = tf.trainable_variables()\n",
    "variables_names = [v.name for v in tf.trainable_variables()]\n",
    "\n",
    "targetOps = updateTargetGraph(trainables[0:24],tau)\n",
    "targetOpsM = updateTargetMGraph(trainables[24:52], tau)\n",
    "targetOpsY = updateTargetGraph(trainables[52:76],tau)\n",
    "targetOpsMY = updateTargetMGraph(trainables[76:104],tau)\n",
    "\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2959
    },
    "colab_type": "code",
    "id": "trttsrde7ZAm",
    "outputId": "ad327a2e-5fab-42fc-e686-107483c32806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 2.36\n",
      "784 4.333333333333333\n",
      "800 2.36\n",
      "800 4.8\n",
      "1584 0.64\n",
      "1584 11.52\n",
      "1600 0.64\n",
      "1600 10.88\n",
      "2384 1.12\n",
      "2384 10.88\n",
      "2400 1.12\n",
      "2400 11.52\n",
      "3184 1.52\n",
      "3184 8.32\n",
      "3200 1.52\n",
      "3200 8.32\n",
      "3984 0.72\n",
      "3984 8.08\n",
      "4000 0.72\n",
      "4000 7.92\n",
      "4784 1.24\n",
      "4784 10.16\n",
      "4800 1.24\n",
      "4800 10.32\n",
      "5584 1.4\n",
      "5584 10.32\n",
      "5600 1.4\n",
      "5600 9.96\n",
      "6384 0.0\n",
      "6384 8.76\n",
      "6400 0.0\n",
      "6400 9.12\n",
      "7184 0.64\n",
      "7184 7.16\n",
      "7200 0.64\n",
      "7200 7.16\n",
      "7984 2.2\n",
      "7984 10.24\n",
      "8000 2.2\n",
      "8000 9.6\n",
      "8784 1.08\n",
      "8784 9.44\n",
      "8800 1.08\n",
      "8800 10.08\n",
      "9584 0.92\n",
      "9584 10.24\n",
      "9600 0.92\n",
      "9600 10.24\n",
      "10384 1.8\n",
      "10384 13.44\n",
      "10400 1.8\n",
      "10400 13.44\n",
      "11184 1.2\n",
      "11184 8.96\n",
      "11200 1.2\n",
      "11200 8.32\n",
      "11984 1.08\n",
      "11984 9.6\n",
      "12000 1.08\n",
      "12000 9.6\n",
      "12784 0.64\n",
      "12784 9.6\n",
      "12800 0.64\n",
      "12800 9.6\n",
      "13584 1.04\n",
      "13584 8.96\n",
      "13600 1.04\n",
      "13600 8.96\n",
      "14384 0.6\n",
      "14384 8.32\n",
      "14400 0.6\n",
      "14400 8.96\n",
      "15184 0.04\n",
      "15184 10.88\n",
      "15200 0.04\n",
      "15200 10.88\n",
      "15984 0.72\n",
      "15984 10.24\n",
      "16000 0.72\n",
      "16000 9.6\n",
      "16784 0.64\n",
      "16784 10.24\n",
      "16800 0.64\n",
      "16800 10.24\n",
      "17584 0.36\n",
      "17584 9.6\n",
      "17600 0.36\n",
      "17600 10.24\n",
      "18384 2.12\n",
      "18384 8.96\n",
      "18400 2.12\n",
      "18400 8.32\n",
      "19184 1.16\n",
      "19184 7.68\n",
      "19200 1.16\n",
      "19200 7.68\n",
      "19984 0.88\n",
      "19984 10.24\n",
      "20000 0.88\n",
      "20000 10.88\n",
      "20784 0.2\n",
      "20784 10.88\n",
      "20800 0.2\n",
      "20800 10.88\n",
      "21584 1.2\n",
      "21584 11.52\n",
      "21600 1.2\n",
      "21600 11.52\n",
      "22384 0.2\n",
      "22384 9.6\n",
      "22400 0.2\n",
      "22400 9.6\n",
      "23184 1.32\n",
      "23184 10.88\n",
      "23200 1.32\n",
      "23200 10.24\n",
      "23984 0.92\n",
      "23984 11.52\n",
      "24000 0.92\n",
      "24000 12.16\n",
      "24784 0.16\n",
      "24784 8.96\n",
      "24800 0.16\n",
      "24800 8.32\n",
      "25584 0.76\n",
      "25584 11.52\n",
      "25600 0.76\n",
      "25600 12.16\n",
      "26384 1.8\n",
      "26384 11.52\n",
      "26400 1.8\n",
      "26400 10.88\n",
      "27184 2.56\n",
      "27184 10.24\n",
      "27200 2.56\n",
      "27200 10.88\n",
      "27984 0.88\n",
      "27984 10.24\n",
      "28000 0.88\n",
      "28000 10.24\n",
      "28784 0.4\n",
      "28784 7.68\n",
      "28800 0.4\n",
      "28800 7.08\n",
      "29584 1.92\n",
      "29584 10.52\n",
      "29600 1.92\n",
      "29600 11.12\n",
      "30384 0.04\n",
      "30384 8.32\n",
      "30400 0.04\n",
      "30400 7.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1fd913e51ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_eps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmainQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainMN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdateQandM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyBuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainMN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmainQNY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainMNY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdateQandM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyBufferY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQNY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainMNY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m#print('model updated')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7ca61bc91f73>\u001b[0m in \u001b[0;36mupdateQandM\u001b[0;34m(myBuffer, mainQN, mainMN)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0mdoubleQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0mtargetQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdoubleQ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mend_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainLength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainMN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainMN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainMN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mmainMN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainLength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mmainMN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainMN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Here we are learning mainQ and mainM over the x-coordinates.\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "myBufferY = experience_buffer()\n",
    "total_steps = 0\n",
    "total_eps = 0\n",
    "rList = []\n",
    "rListY = []\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  for ieps in range(200000):\n",
    "    \n",
    "    ### run an episode for x coord\n",
    "    if ieps%2 == 0:\n",
    "      sse = SSenvReal('x')\n",
    "      sse.max_epLength = max_epLength\n",
    "      epbf,rew = run_episode(sse,mainQN,mainMN,sess,'x')\n",
    "      bufferArray = np.array(epbf)\n",
    "      episodeBuffer = list(zip(bufferArray))\n",
    "      myBuffer.add(episodeBuffer)\n",
    "      rList.append(rew)\n",
    "\n",
    "    else:    \n",
    "    ### run an episode for Y coord\n",
    "      sse = SSenvReal('y')\n",
    "      sse.max_epLength = max_epLength\n",
    "      epbf,rew = run_episode(sse,mainQNY,mainMNY,sess,'y')\n",
    "      bufferArray = np.array(epbf)\n",
    "      episodeBuffer = list(zip(bufferArray))\n",
    "      myBufferY.add(episodeBuffer)\n",
    "      rListY.append(rew)\n",
    "\n",
    "    sse.index = sse.index + 1\n",
    "    total_steps += 16\n",
    "    total_eps += 1\n",
    "    \n",
    "    if total_eps % 5 == 0:\n",
    "      updateTarget(targetOps,sess)\n",
    "      updateTarget(targetOpsM,sess)\n",
    "      updateTarget(targetOpsY,sess)\n",
    "      updateTarget(targetOpsMY,sess)\n",
    "      #print('target updated')\n",
    "    \n",
    "    if total_eps > 20:\n",
    "      if total_eps % 2 == 0:\n",
    "        mainQN, mainMN = updateQandM(myBuffer,mainQN,mainMN)\n",
    "        mainQNY, mainMNY = updateQandM(myBufferY,mainQNY,mainMNY)\n",
    "        #print('model updated')\n",
    "    \n",
    "    if (len(rList) % summaryLength) == 0 and len(rList) != 0:\n",
    "      print (total_steps,np.mean(rList[-summaryLength:]))\n",
    "      print (total_steps,np.mean(rListY[-summaryLength:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uX-GPSyIv_Tp"
   },
   "outputs": [],
   "source": [
    "### What needs to be done ### \n",
    "\n",
    "# Use the function run_episodes to evaluate the policy learned on multi-person tracking (maybe also separate a small test data beforehand)\n",
    "# Use the function get_cover_reward to get a baseline and get learning curve and performance on multi person tracking\n",
    "# Get same performance for random policy (that is Q networks are never updated)\n",
    "# Finally, multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFi3o8YoLwCM"
   },
   "outputs": [],
   "source": [
    "def get_eval_action(mainQNX,mainQNY,state,sess,prev_obs):\n",
    "\n",
    "  if len(prev_obs)==1:\n",
    "    Qx,stateQx = sess.run([mainQNX.Qout, mainQNX.rnn_state],\\\n",
    "          feed_dict={mainQNX.scalarInput:prev_obs,mainQNX.trainLength:1,mainQNX.state_in:state,mainQNX.batch_size:1})\n",
    "    Qy,stateQy = sess.run([mainQNY.Qout, mainQNY.rnn_state],\\\n",
    "          feed_dict={mainQNY.scalarInput:prev_obs,mainQNY.trainLength:1,mainQNY.state_in:state,mainQNY.batch_size:1})\n",
    "    Q = Qx + Qy\n",
    "    a = np.argmax(Q,1)\n",
    "    a = a[0]\n",
    "    return a,stateQx,stateQy\n",
    "  \n",
    "  \n",
    "def run_evaluation_episode(mainQN,mainMN,mainQNY,mainMNY,sess):\n",
    "  \n",
    "  sse1 = SSenvReal()\n",
    "  sse2 = SSenvReal()\n",
    "  \n",
    "  prev_obsx = np.reshape([0]*(sse.nStates+sse.nActions),[1,31])\n",
    "  prev_obsy = np.reshape([0]*(sse.nStates+sse.nActions),[1,31])\n",
    "  state = (np.zeros([1,h_size]),np.zeros([1,h_size])) \n",
    "  stateQx = state\n",
    "  stateQy = state\n",
    "  stateMx = state\n",
    "  stateMy = state\n",
    "  episodeBuffer = []\n",
    "  d = False\n",
    "  rew = 0\n",
    "  for i in range(0,sse.max_epLength):\n",
    "\n",
    "    a,stateQx,stateQy = get_Qaction(mainQN,stateQx,sess,prev_obsx)\n",
    "    a_onehot = np.reshape(np.array([int(i == a) for i in range(sse.nActions)]),[1,sse.nActions])\n",
    "  \n",
    "    ns = (sse.get_cell(sse.get_next_state(sse.track, sse.index)))\n",
    "    ns_cell = sse.discretize_cell(ns)\n",
    "    x_cell = ns_cell[0]\n",
    "    y_cell = ns_cell[1]\n",
    "    \n",
    "    obsx = np.reshape(sse.get_obsX(ns,a),[1,1])\n",
    "    curr_obsx = format_obs(obsx,sse.nStates,a_onehot)\n",
    "    prediction,stateMx = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsx,mainMN.trainLength:1,mainMN.state_in:stateMx,mainMN.batch_size:1})\n",
    "    rx = get_pred_reward(prediction,x_cell)\n",
    "\n",
    "    obsy = np.reshape(sse.get_obsY(ns,a),[1,1])\n",
    "    curr_obsy = format_obs(obsy,sse.nStates,a_onehot)\n",
    "    prediction,stateMy = sess.run([mainMN.prediction,mainMN.rnn_state], feed_dict={mainMN.scalarInput:curr_obsy,mainMN.trainLength:1,mainMN.state_in:stateMy,mainMN.batch_size:1})\n",
    "    ry = get_pred_reward(prediction,x_cell)\n",
    "  \n",
    "    rew = rew + rx + ry\n",
    "    sse1.index = sse1.index + 1\n",
    "    prev_obsx = curr_obsx\n",
    "    prev_obsy = curr_obsy\n",
    "  return rew\n",
    "\n",
    "\n",
    "# Here the idea is to use the trained QX, MX, QY and MY nets to run an evaluation on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzIq5T4SZ14_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DanEvaluation",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
